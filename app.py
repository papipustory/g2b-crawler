import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re

st.set_page_config(
    page_title="ë‚˜ë¼ì¥í„° ì œì•ˆê³µê³  í¬ë¡¤ëŸ¬",
    page_icon="ğŸª",
    layout="wide"
)

st.title("ğŸª ë‚˜ë¼ì¥í„° ì œì•ˆê³µê³  í¬ë¡¤ëŸ¬")

def real_g2b_crawler(query):
    """
    ì‹¤ì œ G2B ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œë„
    """
    try:
        session = requests.Session()
        
        # User-Agent ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        session.headers.update(headers)
        
        st.write(f"ğŸ” '{query}' ê²€ìƒ‰ ì¤‘...")
        
        # G2B ë©”ì¸ í˜ì´ì§€ ì ‘ì†
        main_url = "https://shop.g2b.go.kr/"
        try:
            response = session.get(main_url, timeout=10)
            st.write(f"âœ… G2B ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì„±ê³µ (ìƒíƒœì½”ë“œ: {response.status_code})")
        except Exception as e:
            st.write(f"âŒ G2B ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
            return None
        
        # G2B ê³µê³  ê²€ìƒ‰ í˜ì´ì§€ë“¤ ì‹œë„
        search_urls = [
            "https://www.g2b.go.kr/index.jsp",
            "https://www.g2b.go.kr/pt/menu/selectSubFrame.do?framesrc=/pt/menu/frameTgong.do",
            "https://www.g2b.go.kr/koneps/user/lgnx/notic/bidguidance/BidGuidanceSearchForm.do"
        ]
        
        for url in search_urls:
            try:
                st.write(f"ğŸŒ ì‹œë„ ì¤‘: {url}")
                response = session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ê²€ìƒ‰ í¼ ì°¾ê¸°
                    search_forms = soup.find_all('form')
                    for form in search_forms:
                        inputs = form.find_all('input')
                        for inp in inputs:
                            if inp.get('name') and ('search' in inp.get('name', '').lower() or 'keyword' in inp.get('name', '').lower()):
                                st.write(f"ğŸ” ê²€ìƒ‰ í¼ ë°œê²¬: {inp.get('name')}")
                
            except Exception as e:
                st.write(f"âŒ {url} ì ‘ì† ì‹¤íŒ¨: {e}")
                continue
        
        # ëŒ€ì•ˆ: ë‚˜ë¼ì¥í„° ì‡¼í•‘ëª° ì œì•ˆê³µê³  ì§ì ‘ ì ‘ê·¼ ì‹œë„
        shop_urls = [
            "https://shop.g2b.go.kr/shop/notice.do",
            "https://shop.g2b.go.kr/shop/proposal.do",
            "https://shop.g2b.go.kr/proposal/proposalList.do"
        ]
        
        for url in shop_urls:
            try:
                st.write(f"ğŸ›’ ë‚˜ë¼ì¥í„° ì‡¼í•‘ëª° ì‹œë„: {url}")
                response = session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # í…Œì´ë¸” ì°¾ê¸°
                    tables = soup.find_all('table')
                    for table in tables:
                        rows = table.find_all('tr')
                        if len(rows) > 1:  # í—¤ë” + ë°ì´í„° í–‰ì´ ìˆëŠ” ê²½ìš°
                            st.write(f"ğŸ“Š í…Œì´ë¸” ë°œê²¬: {len(rows)}ê°œ í–‰")
                            
                            # í—¤ë” ì¶”ì¶œ
                            header_row = rows[0]
                            header = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]
                            
                            # ë°ì´í„° ì¶”ì¶œ
                            data_rows = []
                            for row in rows[1:]:
                                cols = row.find_all(['td', 'th'])
                                row_data = [col.get_text().strip() for col in cols]
                                if row_data and any(cell.strip() for cell in row_data):
                                    # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ í–‰ë§Œ í•„í„°ë§
                                    if query.lower() in ' '.join(row_data).lower():
                                        data_rows.append(row_data)
                            
                            if data_rows:
                                st.write(f"âœ… '{query}' ê´€ë ¨ ë°ì´í„° {len(data_rows)}ê°œ ë°œê²¬!")
                                return header, data_rows
                
            except Exception as e:
                st.write(f"âŒ {url} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        st.write("âš ï¸ ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        st.write(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def get_sample_data(query):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    header = ["ê³µê³ ë²ˆí˜¸", "ê³µê³ ëª…", "ê³µê³ ê¸°ê´€", "ê²Œì‹œì¼", "ë§ˆê°ì¼"]
    table_data = [
        ["2024-001", f"{query} ê´€ë ¨ ì‹œìŠ¤í…œ êµ¬ì¶•", "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€", "2024-01-01", "2024-01-31"],
        ["2024-002", f"{query} ì¥ë¹„ êµ¬ë§¤", "ì„œìš¸íŠ¹ë³„ì‹œ", "2024-01-05", "2024-02-05"],
        ["2024-003", f"ìŠ¤ë§ˆíŠ¸ {query} ë„ì… ì‚¬ì—…", "ë¶€ì‚°ê´‘ì—­ì‹œ", "2024-01-10", "2024-02-10"],
        ["2024-004", f"{query} ë³´ì•ˆ ì†”ë£¨ì…˜", "ëŒ€êµ¬ê´‘ì—­ì‹œ", "2024-01-15", "2024-02-15"],
        ["2024-005", f"{query} ê´€ë ¨ ì»¨ì„¤íŒ…", "ì¸ì²œê´‘ì—­ì‹œ", "2024-01-20", "2024-02-20"]
    ]
    return header, table_data

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("â„¹ï¸ í¬ë¡¤ë§ ë°©ì‹")
    crawler_mode = st.selectbox(
        "í¬ë¡¤ë§ ë°©ì‹ ì„ íƒ:",
        ["ì‹¤ì œ í¬ë¡¤ë§ ì‹œë„", "ìƒ˜í”Œ ë°ì´í„°ë§Œ"]
    )
    
    st.markdown("""
    **ì‹¤ì œ í¬ë¡¤ë§ ì‹œë„:**
    - G2B ì‚¬ì´íŠ¸ì— ì‹¤ì œ ì ‘ê·¼
    - ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë”°ë¼ ê²°ê³¼ ìƒì´
    - ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
    
    **ìƒ˜í”Œ ë°ì´í„°ë§Œ:**
    - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
    - ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
    """)

# ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
col1, col2 = st.columns([3, 1])

with col1:
    search_query = st.text_input("ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="ì»´í“¨í„°")

with col2:
    st.write("")
    run_btn = st.button("ğŸš€ í¬ë¡¤ë§ ì‹¤í–‰", type="primary")

# í¬ë¡¤ë§ ì‹¤í–‰
if run_btn and search_query.strip():
    
    progress_bar = st.progress(0)
    status_container = st.container()
    
    with status_container:
        st.write("ğŸ”„ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        progress_bar.progress(20)
        
        if crawler_mode == "ì‹¤ì œ í¬ë¡¤ë§ ì‹œë„":
            with st.expander("ğŸ” í¬ë¡¤ë§ ì§„í–‰ ìƒí™©", expanded=True):
                result = real_g2b_crawler(search_query.strip())
            progress_bar.progress(80)
        else:
            st.write("ğŸ“¦ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            result = get_sample_data(search_query.strip())
            progress_bar.progress(80)
        
        # ì‹¤ì œ í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´
        if not result and crawler_mode == "ì‹¤ì œ í¬ë¡¤ë§ ì‹œë„":
            st.write("ğŸ”„ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            result = get_sample_data(search_query.strip())
        
        progress_bar.progress(90)
        
        if result:
            header, table_data = result
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(table_data, columns=header)
            progress_bar.progress(100)
            
            # ê²°ê³¼ í‘œì‹œ
            st.success(f"âœ… {len(df)}ê°œì˜ ê³µê³ ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            
            # í†µê³„
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ“ˆ ì´ ê³µê³  ìˆ˜", len(df))
            with col2:
                st.metric("ğŸ“Š ì»¬ëŸ¼ ìˆ˜", len(df.columns))
            with col3:
                st.metric("ğŸ” ê²€ìƒ‰ì–´", f'"{search_query}"')
            
            # ë°ì´í„° í‘œì‹œ
            st.dataframe(df, use_container_width=True)
            
            # CSV ë‹¤ìš´ë¡œë“œ
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"g2b_{search_query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
            
            # ì‚¬ìš©ëœ ë°©ì‹ í‘œì‹œ
            if crawler_mode == "ì‹¤ì œ í¬ë¡¤ë§ ì‹œë„":
                st.info("ğŸŒ ì‹¤ì œ í¬ë¡¤ë§ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ“¦ ìƒ˜í”Œ ë°ì´í„°ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.error("âŒ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            progress_bar.progress(100)

elif run_btn and not search_query.strip():
    st.warning("âš ï¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# í‘¸í„°
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
    <small>ğŸª ë‚˜ë¼ì¥í„° ì œì•ˆê³µê³  í¬ë¡¤ëŸ¬ | Made with â¤ï¸ using Streamlit</small>
    </div>
    """, 
    unsafe_allow_html=True
)
